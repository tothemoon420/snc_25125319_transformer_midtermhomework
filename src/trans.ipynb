{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 47173 examples in the training data\n",
      "How are you?\n",
      "Comment êtes-vous?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载 csv 文件\n",
    "df=pd.read_csv(\"files/en2fr.csv\")\n",
    "# 统计数据集中有多少短语对\n",
    "num_examples=len(df)\n",
    "print(f\"there are {num_examples} examples in the training data\")\n",
    "# 打印英文短语示例\n",
    "print(df.iloc[30856][\"en\"])\n",
    "# 打印对应的法语翻译\n",
    "print(df.iloc[30856][\"fr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i</w>', 'don</w>', \"'t</w>\", 'speak</w>', 'fr', 'ench</w>', '.</w>']\n",
      "['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>', '.</w>']\n",
      "['how</w>', 'are</w>', 'you</w>', '?</w>']\n",
      "['comment</w>', 'et', 'es-vous</w>', '?</w>']\n"
     ]
    }
   ],
   "source": [
    "# 导入预训练分词器\n",
    "from transformers import XLMTokenizer\n",
    "\n",
    "tokenizer = XLMTokenizer.from_pretrained(\"xlm-clm-enfr-1024\")\n",
    "# 使用分词器对英文句子进行分词\n",
    "tokenized_en=tokenizer.tokenize(\"I don't speak French.\")\n",
    "print(tokenized_en)\n",
    "# 对法语句子进行分词\n",
    "tokenized_fr=tokenizer.tokenize(\"Je ne parle pas français.\")\n",
    "print(tokenized_fr)\n",
    "print(tokenizer.tokenize(\"How are you?\"))\n",
    "print(tokenizer.tokenize(\"Comment êtes-vous?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# 从训练数据集中获取所有英文句子\n",
    "en=df[\"en\"].tolist()\n",
    "# 对所有英文句子进行分词\n",
    "en_tokens=[[\"BOS\"]+tokenizer.tokenize(x)+[\"EOS\"] for x in en]        \n",
    "PAD=0\n",
    "UNK=1\n",
    "word_count=Counter()\n",
    "for sentence in en_tokens:\n",
    "    for word in sentence:\n",
    "        word_count[word]+=1\n",
    "# 统计词频\n",
    "frequency=word_count.most_common(50000)        \n",
    "total_en_words=len(frequency)+2\n",
    "# 创建字典，将词元映射到索引\n",
    "en_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}\n",
    "en_word_dict[\"PAD\"]=PAD\n",
    "en_word_dict[\"UNK\"]=UNK\n",
    "# 创建字典，将索引映射到词元\n",
    "en_idx_dict={v:k for k,v in en_word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 100, 38, 377, 476, 574, 5]\n"
     ]
    }
   ],
   "source": [
    "enidx=[en_word_dict.get(i,UNK) for i in tokenized_en]   \n",
    "print(enidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i</w>', 'don</w>', \"'t</w>\", 'speak</w>', 'fr', 'ench</w>', '.</w>']\n",
      "i don't speak french. \n"
     ]
    }
   ],
   "source": [
    "# 将索引转换为词元\n",
    "entokens=[en_idx_dict.get(i,\"UNK\") for i in enidx]   \n",
    "print(entokens)\n",
    "# 将词元连接成一个字符串\n",
    "en_phrase=\"\".join(entokens)\n",
    "# 用空格替换分隔符\n",
    "en_phrase=en_phrase.replace(\"</w>\",\" \")\n",
    "for x in '''?:;.,'(\"-!&)%''':\n",
    "    # 去除标点符号前的空格\n",
    "    en_phrase=en_phrase.replace(f\" {x}\",f\"{x}\")   \n",
    "print(en_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[157, 17, 22, 26]\n",
      "['how</w>', 'are</w>', 'you</w>', '?</w>']\n",
      "how are you? \n"
     ]
    }
   ],
   "source": [
    "tokens=['how</w>', 'are</w>', 'you</w>', '?</w>']\n",
    "indexes=[en_word_dict.get(i,UNK) for i in tokens]   \n",
    "print(indexes)\n",
    "tokens=[en_idx_dict.get(i,\"UNK\") for i in indexes]   \n",
    "print(tokens)\n",
    "phrase=\"\".join(tokens)\n",
    "phrase=phrase.replace(\"</w>\",\" \")\n",
    "for x in '''?:;.,'(\"-!&)%''':\n",
    "    phrase=phrase.replace(f\" {x}\",f\"{x}\")   \n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有法语句子进行分词\n",
    "fr=df[\"fr\"].tolist()       \n",
    "fr_tokens=[[\"BOS\"]+tokenizer.tokenize(x)+[\"EOS\"] for x in fr] \n",
    "word_count=Counter()\n",
    "for sentence in fr_tokens:\n",
    "    for word in sentence:\n",
    "        word_count[word]+=1\n",
    "# 统计法语词频\n",
    "frequency=word_count.most_common(50000)        \n",
    "total_fr_words=len(frequency)+2\n",
    "# 创建一个字典，将法语词元映射到索引\n",
    "fr_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}\n",
    "fr_word_dict[\"PAD\"]=PAD\n",
    "fr_word_dict[\"UNK\"]=UNK\n",
    "# 创建一个字典，将索引映射到法语词元\n",
    "fr_idx_dict={v:k for k,v in fr_word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 40, 231, 32, 726, 370, 4]\n"
     ]
    }
   ],
   "source": [
    "fridx=[fr_word_dict.get(i,UNK) for i in tokenized_fr]   \n",
    "print(fridx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>', '.</w>']\n",
      "je ne parle pas francais. \n"
     ]
    }
   ],
   "source": [
    "frtokens=[fr_idx_dict.get(i,\"UNK\") for i in fridx]   \n",
    "print(frtokens)\n",
    "fr_phrase=\"\".join(frtokens)\n",
    "fr_phrase=fr_phrase.replace(\"</w>\",\" \")\n",
    "for x in '''?:;.,'(\"-!&)%''':\n",
    "    fr_phrase=fr_phrase.replace(f\" {x}\",f\"{x}\")  \n",
    "print(fr_phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[452, 61, 742, 30]\n",
      "['comment</w>', 'et', 'es-vous</w>', '?</w>']\n",
      "comment etes-vous? \n"
     ]
    }
   ],
   "source": [
    "tokens=['comment</w>', 'et', 'es-vous</w>', '?</w>']\n",
    "indexes=[fr_word_dict.get(i,UNK) for i in tokens]   \n",
    "print(indexes)\n",
    "tokens=[fr_idx_dict.get(i,\"UNK\") for i in indexes]   \n",
    "print(tokens)\n",
    "phrase=\"\".join(tokens)\n",
    "phrase=phrase.replace(\"</w>\",\" \")\n",
    "for x in '''?:;.,'(\"-!&)%''':\n",
    "    phrase=phrase.replace(f\" {x}\",f\"{x}\")   \n",
    "print(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"files/dict.p\",\"wb\") as fb:\n",
    "    pickle.dump((en_word_dict,en_idx_dict,\n",
    "                 fr_word_dict,fr_idx_dict),fb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_en_ids=[[en_word_dict.get(w,1) for w in s] for s in en_tokens]\n",
    "out_fr_ids=[[fr_word_dict.get(w,1) for w in s] for s in fr_tokens]\n",
    "sorted_ids=sorted(range(len(out_en_ids)),\n",
    "                  key=lambda x:len(out_en_ids[x]))\n",
    "out_en_ids=[out_en_ids[x] for x in sorted_ids]\n",
    "out_fr_ids=[out_fr_ids[x] for x in sorted_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_size=128\n",
    "idx_list=np.arange(0,len(en_tokens),batch_size)\n",
    "np.random.shuffle(idx_list)\n",
    "\n",
    "batch_indexs=[]\n",
    "for idx in idx_list:\n",
    "    batch_indexs.append(np.arange(idx,min(len(en_tokens),\n",
    "                                          idx+batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    L = [len(x) for x in X]\n",
    "    # 找出批次中最长序列的长度\n",
    "    ML = max(L)\n",
    "    # 如果批次短于最长序列，则在序列末尾填充 0\n",
    "    padded_seq = np.array([np.concatenate([x, [padding] * (ML - len(x))])\n",
    "        if len(x) < ML else x for x in X])\n",
    "    return padded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import Batch\n",
    "\n",
    "batches=[]\n",
    "for b in batch_indexs:\n",
    "    batch_en=[out_en_ids[x] for x in b]\n",
    "    batch_fr=[out_fr_ids[x] for x in b]\n",
    "    batch_en=seq_padding(batch_en)\n",
    "    batch_fr=seq_padding(batch_fr)\n",
    "    batches.append(Batch(batch_en,batch_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 11055 distinct English tokens\n",
      "there are 11239 distinct French tokens\n"
     ]
    }
   ],
   "source": [
    "src_vocab = len(en_word_dict)\n",
    "tgt_vocab = len(fr_word_dict)\n",
    "print(f\"there are {src_vocab} distinct English tokens\")\n",
    "print(f\"there are {tgt_vocab} distinct French tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of positional encoding is torch.Size([1, 8, 256])\n",
      "tensor([[[ 0.0000e+00,  1.1111e+00,  0.0000e+00,  ...,  1.1111e+00,\n",
      "           0.0000e+00,  1.1111e+00],\n",
      "         [ 9.3497e-01,  6.0034e-01,  8.9107e-01,  ...,  1.1111e+00,\n",
      "           1.1940e-04,  1.1111e+00],\n",
      "         [ 1.0103e+00, -4.6239e-01,  1.0646e+00,  ...,  1.1111e+00,\n",
      "           2.3880e-04,  1.1111e+00],\n",
      "         ...,\n",
      "         [-1.0655e+00,  3.1518e-01, -1.1091e+00,  ...,  1.1111e+00,\n",
      "           5.9700e-04,  0.0000e+00],\n",
      "         [-3.1046e-01,  1.0669e+00, -7.1559e-01,  ...,  1.1111e+00,\n",
      "           7.1640e-04,  1.1111e+00],\n",
      "         [ 0.0000e+00,  8.3767e-01,  2.5419e-01,  ...,  1.1111e+00,\n",
      "           8.3581e-04,  1.1111e+00]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from util import PositionalEncoding\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 实例化 PositionalEncoding() 类并将模型维度设置为 256\n",
    "pe = PositionalEncoding(256, 0.1)\n",
    "# 创建一个词嵌入并用零填充\n",
    "x = torch.zeros(1, 8, 256).to(DEVICE)\n",
    "# 通过将位置编码添加到词嵌入来计算输入嵌入\n",
    "y = pe.forward(x)\n",
    "print(f\"the shape of positional encoding is {y.shape}\")\n",
    "# 打印输入嵌入，由于词嵌入被设置为零，因此它与位置编码相同\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import create_model\n",
    "\n",
    "model = create_model(src_vocab, tgt_vocab, N=6,\n",
    "    d_model=256, d_ff=1024, h=8, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import NoamOpt\n",
    "\n",
    "optimizer = NoamOpt(256, 1, 2000, torch.optim.Adam(\n",
    "    model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import LabelSmoothing, SimpleLossCompute\n",
    "\n",
    "criterion = LabelSmoothing(tgt_vocab, \n",
    "                           padding_idx=0, smoothing=0.1)\n",
    "loss_func = SimpleLossCompute(\n",
    "            model.generator, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 5.837538242340088\n",
      "Epoch 1, average loss: 3.6549735069274902\n",
      "Epoch 2, average loss: 2.867757797241211\n",
      "Epoch 3, average loss: 2.248039722442627\n",
      "Epoch 4, average loss: 1.8180747032165527\n",
      "Epoch 5, average loss: 1.5864763259887695\n",
      "Epoch 6, average loss: 1.4054176807403564\n",
      "Epoch 7, average loss: 1.2680836915969849\n",
      "Epoch 8, average loss: 1.162902593612671\n",
      "Epoch 9, average loss: 1.0832992792129517\n",
      "Epoch 10, average loss: 1.016732931137085\n",
      "Epoch 11, average loss: 0.9556108713150024\n",
      "Epoch 12, average loss: 0.9017373919487\n",
      "Epoch 13, average loss: 0.8607497215270996\n",
      "Epoch 14, average loss: 0.8227623701095581\n",
      "Epoch 15, average loss: 0.7802281975746155\n",
      "Epoch 16, average loss: 0.7497196793556213\n",
      "Epoch 17, average loss: 0.7211118340492249\n",
      "Epoch 18, average loss: 0.6901364922523499\n",
      "Epoch 19, average loss: 0.668834924697876\n",
      "Epoch 20, average loss: 0.6482908725738525\n",
      "Epoch 21, average loss: 0.626395583152771\n",
      "Epoch 22, average loss: 0.6073558926582336\n",
      "Epoch 23, average loss: 0.5877125263214111\n",
      "Epoch 24, average loss: 0.5696532726287842\n",
      "Epoch 25, average loss: 0.555048406124115\n",
      "Epoch 26, average loss: 0.5421203374862671\n",
      "Epoch 27, average loss: 0.5281668305397034\n",
      "Epoch 28, average loss: 0.5161176919937134\n",
      "Epoch 29, average loss: 0.5047862529754639\n",
      "Epoch 30, average loss: 0.4925852119922638\n",
      "Epoch 31, average loss: 0.4813229739665985\n",
      "Epoch 32, average loss: 0.4728938043117523\n",
      "Epoch 33, average loss: 0.4649004340171814\n",
      "Epoch 34, average loss: 0.45988401770591736\n",
      "Epoch 35, average loss: 0.4463028609752655\n",
      "Epoch 36, average loss: 0.4373738765716553\n",
      "Epoch 37, average loss: 0.43195560574531555\n",
      "Epoch 38, average loss: 0.4245850443840027\n",
      "Epoch 39, average loss: 0.4160899817943573\n",
      "Epoch 40, average loss: 0.40978100895881653\n",
      "Epoch 41, average loss: 0.40424105525016785\n",
      "Epoch 42, average loss: 0.3982260823249817\n",
      "Epoch 43, average loss: 0.3933359682559967\n",
      "Epoch 44, average loss: 0.38766244053840637\n",
      "Epoch 45, average loss: 0.3824770152568817\n",
      "Epoch 46, average loss: 0.37867268919944763\n",
      "Epoch 47, average loss: 0.3742208778858185\n",
      "Epoch 48, average loss: 0.3682709038257599\n",
      "Epoch 49, average loss: 0.3645106256008148\n",
      "Epoch 50, average loss: 0.3599468469619751\n",
      "Epoch 51, average loss: 0.3559485673904419\n",
      "Epoch 52, average loss: 0.3545718789100647\n",
      "Epoch 53, average loss: 0.34997671842575073\n",
      "Epoch 54, average loss: 0.3451518416404724\n",
      "Epoch 55, average loss: 0.3414178490638733\n",
      "Epoch 56, average loss: 0.33859017491340637\n",
      "Epoch 57, average loss: 0.33456045389175415\n",
      "Epoch 58, average loss: 0.331927090883255\n",
      "Epoch 59, average loss: 0.32795843482017517\n",
      "Epoch 60, average loss: 0.3252517879009247\n",
      "Epoch 61, average loss: 0.3238224685192108\n",
      "Epoch 62, average loss: 0.3202427923679352\n",
      "Epoch 63, average loss: 0.31952667236328125\n",
      "Epoch 64, average loss: 0.3152613341808319\n",
      "Epoch 65, average loss: 0.3125666081905365\n",
      "Epoch 66, average loss: 0.3099941909313202\n",
      "Epoch 67, average loss: 0.3076830208301544\n",
      "Epoch 68, average loss: 0.3060295879840851\n",
      "Epoch 69, average loss: 0.3037734925746918\n",
      "Epoch 70, average loss: 0.30105945467948914\n",
      "Epoch 71, average loss: 0.2991315424442291\n",
      "Epoch 72, average loss: 0.2975235879421234\n",
      "Epoch 73, average loss: 0.2952406406402588\n",
      "Epoch 74, average loss: 0.29349231719970703\n",
      "Epoch 75, average loss: 0.2909072935581207\n",
      "Epoch 76, average loss: 0.2895205616950989\n",
      "Epoch 77, average loss: 0.28805768489837646\n",
      "Epoch 78, average loss: 0.28710296750068665\n",
      "Epoch 79, average loss: 0.2850375175476074\n",
      "Epoch 80, average loss: 0.28310054540634155\n",
      "Epoch 81, average loss: 0.2810268700122833\n",
      "Epoch 82, average loss: 0.279499888420105\n",
      "Epoch 83, average loss: 0.2784571349620819\n",
      "Epoch 84, average loss: 0.27605485916137695\n",
      "Epoch 85, average loss: 0.2741270959377289\n",
      "Epoch 86, average loss: 0.2731782793998718\n",
      "Epoch 87, average loss: 0.27239876985549927\n",
      "Epoch 88, average loss: 0.2700810730457306\n",
      "Epoch 89, average loss: 0.26923519372940063\n",
      "Epoch 90, average loss: 0.26750707626342773\n",
      "Epoch 91, average loss: 0.26658347249031067\n",
      "Epoch 92, average loss: 0.2653484344482422\n",
      "Epoch 93, average loss: 0.2637450695037842\n",
      "Epoch 94, average loss: 0.2626246511936188\n",
      "Epoch 95, average loss: 0.2615446448326111\n",
      "Epoch 96, average loss: 0.26006078720092773\n",
      "Epoch 97, average loss: 0.2589542269706726\n",
      "Epoch 98, average loss: 0.25900301337242126\n",
      "Epoch 99, average loss: 0.2561013698577881\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    tloss=0\n",
    "    tokens=0\n",
    "    for batch in batches:\n",
    "        # 使用 Transformer 进行预测\n",
    "        out = model(batch.src, batch.trg, \n",
    "                    batch.src_mask, batch.trg_mask)\n",
    "        # 计算损失并调整模型参数\n",
    "        loss = loss_func(out, batch.trg_y, batch.ntokens)\n",
    "        tloss += loss\n",
    "        # 计算批次中的词元数量\n",
    "        tokens += batch.ntokens\n",
    "    print(f\"Epoch {epoch}, average loss: {tloss/tokens}\")\n",
    "# 保存训练好的模型权重\n",
    "torch.save(model.state_dict(),\"files/en2fr.pth\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(eng):\n",
    "    tokenized_en=tokenizer.tokenize(eng)\n",
    "    tokenized_en=[\"BOS\"]+tokenized_en+[\"EOS\"]\n",
    "    enidx=[en_word_dict.get(i,UNK) for i in tokenized_en]  \n",
    "    src=torch.tensor(enidx).long().to(DEVICE).unsqueeze(0)\n",
    "    src_mask=(src!=0).unsqueeze(-2)\n",
    "    # 使用编码器将英文短语转换为向量表示\n",
    "    memory=model.encode(src,src_mask)\n",
    "    # 使用解码器预测下一个词元\n",
    "    start_symbol=fr_word_dict[\"BOS\"]\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    translation=[]\n",
    "    for i in range(100):\n",
    "        out = model.decode(memory,src_mask,ys,\n",
    "        subsequent_mask(ys.size(1)).type_as(src.data))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(\n",
    "            src.data).fill_(next_word)], dim=1)\n",
    "        sym = fr_idx_dict[ys[0, -1].item()]\n",
    "        # 当下一个词元为 “EOS” 时停止翻译\n",
    "        if sym != 'EOS':\n",
    "            translation.append(sym)\n",
    "        else:\n",
    "            break\n",
    "    # 将预测的词元连接起来形成法语句子\n",
    "    trans=\"\".join(translation)\n",
    "    trans=trans.replace(\"</w>\",\" \") \n",
    "    for x in '''?:;.,'(\"-!&)%''':\n",
    "        trans=trans.replace(f\" {x}\",f\"{x}\")    \n",
    "    print(trans)\n",
    "    return trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m运行具有“c:\\Users\\asus\\AppData\\Local\\Programs\\Python\\Python39\\python.exe”的单元格需要ipykernel包。\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/asus/AppData/Local/Programs/Python/Python39/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from util import subsequent_mask\n",
    "\n",
    "with open(\"files/dict.p\",\"rb\") as fb:\n",
    "    en_word_dict,en_idx_dict,\\\n",
    "    fr_word_dict,fr_idx_dict=pickle.load(fb)\n",
    "trained_weights=torch.load(\"files/en2fr.pth\",\n",
    "                           map_location=DEVICE,\n",
    "                           weights_only=False)\n",
    "model.load_state_dict(trained_weights)\n",
    "model.eval()\n",
    "eng = \"Today is a beautiful day!\"\n",
    "translated_fr = translate(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un petit garcon en jeans grimpe un petit arbre tandis qu'un autre enfant regarde. \n"
     ]
    }
   ],
   "source": [
    "eng = \"A little boy in jeans climbs a small tree while another child looks on.\"\n",
    "translated_fr = translate(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je ne parle pas francais. \n"
     ]
    }
   ],
   "source": [
    "eng = \"I don't speak French.\"\n",
    "translated_fr = translate(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je ne parle pas francais. \n"
     ]
    }
   ],
   "source": [
    "eng = \"I do not speak French.\"\n",
    "translated_fr = translate(eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "j'aime faire du ski dans l'hiver! \n",
      "comment etes-vous? \n"
     ]
    }
   ],
   "source": [
    "eng = \"I love skiing in the winter!\"\n",
    "translated_fr = translate(eng)\n",
    "eng = \"How are you?\"\n",
    "translated_fr = translate(eng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
